{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sabarinathan612/machine_learning_project/blob/main/mini_chatbot_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq2kjPBUJ-lX",
        "outputId": "60d1f01d-e475-4eff-d7f3-d8882a5ce02d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 5)]                  0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 22)]                 0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 5, 256)               10496     ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, 22, 256)              10496     ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 [(None, 512),                1574912   ['embedding[0][0]']           \n",
            "                              (None, 512),                                                        \n",
            "                              (None, 512)]                                                        \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               [(None, 22, 512),            1574912   ['embedding_1[0][0]',         \n",
            "                              (None, 512),                           'lstm[0][1]',                \n",
            "                              (None, 512)]                           'lstm[0][2]']                \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 22, 41)               21033     ['lstm_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3191849 (12.18 MB)\n",
            "Trainable params: 3191849 (12.18 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 3.7098 - val_loss: 3.5560\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 3.6074 - val_loss: 3.3575\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 384ms/step - loss: 3.4736 - val_loss: 2.9836\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 1s 506ms/step - loss: 3.2282 - val_loss: 2.2059\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 1s 604ms/step - loss: 2.7202 - val_loss: 1.5658\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 2.4988 - val_loss: 1.5225\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 461ms/step - loss: 2.3512 - val_loss: 1.5590\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 425ms/step - loss: 2.2594 - val_loss: 1.5464\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 2.2533 - val_loss: 1.5023\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 2.2310 - val_loss: 1.4554\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 2.2121 - val_loss: 1.4265\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 328ms/step - loss: 2.2030 - val_loss: 1.4150\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 2.1868 - val_loss: 1.4128\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 2.1526 - val_loss: 1.4199\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 334ms/step - loss: 2.1118 - val_loss: 1.4332\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 2.0767 - val_loss: 1.4434\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 2.0467 - val_loss: 1.4437\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 335ms/step - loss: 2.0176 - val_loss: 1.4361\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 1.9960 - val_loss: 1.4367\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 1.9873 - val_loss: 1.4682\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 1.9610 - val_loss: 1.5115\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 319ms/step - loss: 1.9229 - val_loss: 1.5122\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 1.8693 - val_loss: 1.4818\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.8062 - val_loss: 1.4877\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 420ms/step - loss: 1.7643 - val_loss: 1.5597\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 419ms/step - loss: 1.7277 - val_loss: 1.6875\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 1.6702 - val_loss: 1.8970\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 1.6444 - val_loss: 1.9264\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 454ms/step - loss: 1.5590 - val_loss: 2.0308\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 437ms/step - loss: 1.5213 - val_loss: 2.1515\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 336ms/step - loss: 1.4349 - val_loss: 2.3097\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 1.4030 - val_loss: 2.3166\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 1.3352 - val_loss: 2.4320\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 326ms/step - loss: 1.2800 - val_loss: 2.6053\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 339ms/step - loss: 1.2067 - val_loss: 2.7450\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 1.1294 - val_loss: 2.9017\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 1s 622ms/step - loss: 1.0828 - val_loss: 3.0494\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 1s 582ms/step - loss: 1.1541 - val_loss: 3.2146\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 1.0462 - val_loss: 3.3112\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 0.9372 - val_loss: 3.3634\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 461ms/step - loss: 0.9100 - val_loss: 3.4372\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 0.8470 - val_loss: 3.5299\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 0.8228 - val_loss: 3.5769\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 459ms/step - loss: 0.7679 - val_loss: 3.6470\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 1s 578ms/step - loss: 0.7456 - val_loss: 3.7434\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 0.7085 - val_loss: 3.7635\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 0.6738 - val_loss: 3.9230\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 1s 535ms/step - loss: 0.6758 - val_loss: 3.8923\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 0.7083 - val_loss: 3.9794\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 1s 507ms/step - loss: 0.6041 - val_loss: 4.1534\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 0.5661 - val_loss: 4.0421\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 1s 611ms/step - loss: 0.5859 - val_loss: 4.0435\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 0.5146 - val_loss: 4.0612\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 1s 538ms/step - loss: 0.4850 - val_loss: 4.1151\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 1s 568ms/step - loss: 0.4891 - val_loss: 4.2180\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 0.4281 - val_loss: 4.2882\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 1s 567ms/step - loss: 0.4359 - val_loss: 4.2784\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 1s 553ms/step - loss: 0.4258 - val_loss: 4.3048\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 0.3706 - val_loss: 4.3473\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 1s 588ms/step - loss: 0.3753 - val_loss: 4.3699\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 1s 571ms/step - loss: 0.3617 - val_loss: 4.4114\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 457ms/step - loss: 0.3228 - val_loss: 4.4422\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 0.3343 - val_loss: 4.4451\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 500ms/step - loss: 0.3115 - val_loss: 4.4800\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 0.2759 - val_loss: 4.5395\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 0.2892 - val_loss: 4.5630\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 351ms/step - loss: 0.2786 - val_loss: 4.6083\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 0.2393 - val_loss: 4.6224\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 0.2480 - val_loss: 4.6128\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 0.2453 - val_loss: 4.6360\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 453ms/step - loss: 0.2081 - val_loss: 4.6718\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 0.2096 - val_loss: 4.6943\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 0.2174 - val_loss: 4.7571\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 500ms/step - loss: 0.1875 - val_loss: 4.7715\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 0.1757 - val_loss: 4.7673\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 450ms/step - loss: 0.1843 - val_loss: 4.7951\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 460ms/step - loss: 0.1660 - val_loss: 4.8034\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 0.1504 - val_loss: 4.8193\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 443ms/step - loss: 0.1557 - val_loss: 4.8547\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 0.1481 - val_loss: 4.8755\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 0.1315 - val_loss: 4.9034\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 454ms/step - loss: 0.1300 - val_loss: 4.9489\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 449ms/step - loss: 0.1290 - val_loss: 4.9532\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 460ms/step - loss: 0.1168 - val_loss: 4.9676\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 1s 613ms/step - loss: 0.1105 - val_loss: 4.9856\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 1s 628ms/step - loss: 0.1106 - val_loss: 4.9903\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 1s 577ms/step - loss: 0.1031 - val_loss: 5.0095\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 1s 706ms/step - loss: 0.0959 - val_loss: 5.0353\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 1s 604ms/step - loss: 0.0952 - val_loss: 5.0504\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 421ms/step - loss: 0.0899 - val_loss: 5.0790\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 382ms/step - loss: 0.0837 - val_loss: 5.1008\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 379ms/step - loss: 0.0826 - val_loss: 5.0971\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 377ms/step - loss: 0.0780 - val_loss: 5.1101\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 385ms/step - loss: 0.0733 - val_loss: 5.1259\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 356ms/step - loss: 0.0724 - val_loss: 5.1376\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 415ms/step - loss: 0.0681 - val_loss: 5.1556\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 396ms/step - loss: 0.0644 - val_loss: 5.1794\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 389ms/step - loss: 0.0635 - val_loss: 5.1894\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 387ms/step - loss: 0.0596 - val_loss: 5.2026\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 371ms/step - loss: 0.0574 - val_loss: 5.2198\n",
            "Start chatting with the bot (type 'exit' to stop)!\n",
            "1/1 [==============================] - 0s 428ms/step\n",
            "1/1 [==============================] - 0s 477ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Bot: hello ! how can i help you ? end\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Bot: i am a chatbot created by sabari end\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "\n",
        "# Sample dataset (input-output pairs)\n",
        "conversations = [\n",
        "    (\"Hi\", \"Hello! How can I help you?\"),\n",
        "    (\"What's your name?\", \"I am a chatbot created by Sabari\"),\n",
        "    (\"How are you?\", \"I'm just a bunch of code, so I don't have feelings, but thanks for asking!\"),\n",
        "    (\"Goodbye\", \"Goodbye! Have a nice day!\"),\n",
        "]\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    return sentence\n",
        "\n",
        "# Preprocess the dataset\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "for input_text, target_text in conversations:\n",
        "    input_texts.append(preprocess_sentence(input_text))\n",
        "    target_texts.append(f'start {preprocess_sentence(target_text)} end')\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "tokenizer.fit_on_texts(input_texts + target_texts)\n",
        "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
        "target_sequences = tokenizer.texts_to_sequences(target_texts)\n",
        "\n",
        "# Pad sequences\n",
        "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, padding='post')\n",
        "target_sequences = tf.keras.preprocessing.sequence.pad_sequences(target_sequences, padding='post')\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_length_input = input_sequences.shape[1]\n",
        "max_length_target = target_sequences.shape[1]\n",
        "\n",
        "# Shift the target sequences by one position\n",
        "target_sequences_shifted = np.zeros(target_sequences.shape)\n",
        "target_sequences_shifted[:, :-1] = target_sequences[:, 1:]\n",
        "target_sequences_shifted[:, -1] = 0  # Padding with zero at the end\n",
        "\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_length_input,))\n",
        "encoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(units, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_length_target,))\n",
        "decoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "\n",
        "model.fit(\n",
        "    [input_sequences, target_sequences],\n",
        "    target_sequences_shifted,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Encoder model\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Decoder model\n",
        "decoder_state_input_h = Input(shape=(units,))\n",
        "decoder_state_input_c = Input(shape=(units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_embedding, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "# Reverse-lookup token index to convert decoded sequences back to text\n",
        "reverse_word_index = dict((i, word) for word, i in tokenizer.word_index.items())\n",
        "\n",
        "# Decode sequence\n",
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = tokenizer.word_index['start']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = reverse_word_index.get(sampled_token_index, '')\n",
        "\n",
        "        decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "        if sampled_word == 'end' or len(decoded_sentence.split()) > max_length_target:\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()\n",
        "\n",
        "# Interactive chat\n",
        "def chat():\n",
        "    print(\"Start chatting with the bot (type 'exit' to stop)!\")\n",
        "    while True:\n",
        "        input_text = input(\"You: \")\n",
        "        if input_text.lower() == 'exit':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        input_seq = tokenizer.texts_to_sequences([preprocess_sentence(input_text)])\n",
        "        input_seq = tf.keras.preprocessing.sequence.pad_sequences(input_seq, maxlen=max_length_input, padding='post')\n",
        "        decoded_sentence = decode_sequence(input_seq)\n",
        "        print(f\"Bot: {decoded_sentence}\")\n",
        "\n",
        "chat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brc-YCAvKDpE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}